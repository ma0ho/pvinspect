{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pyinspect_pytorch_integration.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOa5YRYP+dhAqpUBMaqkfdt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0d5z_vqhTT9-"},"source":["# PVInspect integration with PyTorch\n","\n","PVInspect interfaces directly with PyTorch, since PVInspect `ImageSequence`s can be simply converted into PyTorch datasets. To demonstrate this, let's first set up everything:"]},{"cell_type":"code","metadata":{"id":"BAZ77uMcTQ8c"},"source":["!git clone https://github.com/ma0ho/pvinspect.git\n","%cd pvinspect\n","!git checkout rework\n","!pip install ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"28ZA1FH9TrwN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629900024449,"user_tz":-120,"elapsed":5808,"user":{"displayName":"Mathis Hoffmann","photoUrl":"","userId":"16668827174194590856"}},"outputId":"ac9d8f55-94fe-4f46-a6a3-c2e7b509ddd5"},"source":["import pvinspect as pv\n","from pvinspect.data.image import DType\n","from pvinspect.integration.pytorch.dataset import ClassificationDataset\n","from torchvision.transforms import Compose, RandomHorizontalFlip, RandomVerticalFlip, ToTensor, ToPILImage"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/pvinspect/data/image/show_plugin.py:38: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n","  from tqdm.autonotebook import tqdm\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"RBiWM_HGUIAi"},"source":["Now, we load our sample dataset and only use the training data:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gYIVTJ7OUDbq","executionInfo":{"status":"ok","timestamp":1629900031089,"user_tz":-120,"elapsed":6643,"user":{"displayName":"Mathis Hoffmann","photoUrl":"","userId":"16668827174194590856"}},"outputId":"bc637da4-d04c-40e3-9e6e-d865922fef18"},"source":["seq = pv.datasets.elpv().pandas.query(\"testset == False\")\n","len(seq)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading 1hK_hViiZ1-rHhvI3yGxpC6DSCXAyAFiJ into /usr/local/lib/python3.7/dist-packages/pvinspect/datasets/datasets/20200728_elpv_labels/data.zip... Done.\n","Unzipping...Done.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["2324"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"C3fMmlb4UVsy"},"source":["Note that this has annotations available in form of boolean metra attributes:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"iO59aa5hUPzq","executionInfo":{"status":"ok","timestamp":1629900031092,"user_tz":-120,"elapsed":21,"user":{"displayName":"Mathis Hoffmann","photoUrl":"","userId":"16668827174194590856"}},"outputId":"d77e8628-f97e-4f72-cf5f-7e57b6478cca"},"source":["seq[:3].meta"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>original_filename</th>\n","      <th>modality</th>\n","      <th>defect_probability</th>\n","      <th>wafer</th>\n","      <th>crack</th>\n","      <th>inactive</th>\n","      <th>blob</th>\n","      <th>finger</th>\n","      <th>testset</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>cell0634.png</td>\n","      <td>EL_IMAGE</td>\n","      <td>1.0</td>\n","      <td>poly</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>cell0805.png</td>\n","      <td>EL_IMAGE</td>\n","      <td>0.0</td>\n","      <td>poly</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>cell2285.png</td>\n","      <td>EL_IMAGE</td>\n","      <td>0.0</td>\n","      <td>poly</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  original_filename  modality  defect_probability  ...   blob  finger  testset\n","0      cell0634.png  EL_IMAGE                 1.0  ...  False    True    False\n","1      cell0805.png  EL_IMAGE                 0.0  ...  False   False    False\n","2      cell2285.png  EL_IMAGE                 0.0  ...  False    True    False\n","\n","[3 rows x 9 columns]"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"OOZjw5hOUglk"},"source":["For training, we usually have some data augmentation pipeline. Let's set up a simple one:"]},{"cell_type":"code","metadata":{"id":"MCyotbLXUaxZ","executionInfo":{"status":"ok","timestamp":1629900031093,"user_tz":-120,"elapsed":18,"user":{"displayName":"Mathis Hoffmann","photoUrl":"","userId":"16668827174194590856"}}},"source":["tfms = Compose([ToPILImage(), RandomHorizontalFlip(), RandomVerticalFlip(), ToTensor()])"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yzz_9yxLUosB"},"source":["Now, converting `seq` into a PyTorch classification dataset using the transforms is as simple as:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HoS72hEWUmCz","executionInfo":{"status":"ok","timestamp":1629900031095,"user_tz":-120,"elapsed":18,"user":{"displayName":"Mathis Hoffmann","photoUrl":"","userId":"16668827174194590856"}},"outputId":"b07c37a2-290b-4cce-8c5e-55c301708f74"},"source":["ds = ClassificationDataset(seq, meta_classes=[\"crack\", \"inactive\"], data_transform=tfms)\n","ds"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pvinspect.integration.pytorch.dataset.ClassificationDataset at 0x7f125e02bd10>"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"qtITkSTeU1hB"},"source":["With the `meta_classes` parameter, we specify the meta attributes that should be converted into one-hot class variables. We can now access individual elements from the dataset:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QlqYB2AnUz0S","executionInfo":{"status":"ok","timestamp":1629900031402,"user_tz":-120,"elapsed":321,"user":{"displayName":"Mathis Hoffmann","photoUrl":"","userId":"16668827174194590856"}},"outputId":"f4d5da39-a0a2-405b-dd3a-b4e08a204916"},"source":["ds[0]"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[[0.2941, 0.2941, 0.2941,  ..., 0.2941, 0.2980, 0.2941],\n","          [0.2980, 0.2980, 0.2980,  ..., 0.2902, 0.2941, 0.2980],\n","          [0.3020, 0.3020, 0.3020,  ..., 0.2863, 0.2902, 0.2941],\n","          ...,\n","          [0.3098, 0.3098, 0.3137,  ..., 0.2824, 0.2824, 0.2784],\n","          [0.3098, 0.3098, 0.3098,  ..., 0.2824, 0.2784, 0.2784],\n","          [0.3098, 0.3137, 0.3137,  ..., 0.2784, 0.2784, 0.2784]]]),\n"," tensor([1., 0.]))"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"TdBeJAF5VU-5"},"source":["Here, we see that\n","\n","\n","1.   Image data is corretcly converted into PyTorch tensors, which shows that the transforms are applied\n","2.   Specified classes are converted into a one-hot vector and returned as a second return value\n","\n"]}]}