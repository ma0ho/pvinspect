{"cells":[{"cell_type":"markdown","metadata":{"id":"0d5z_vqhTT9-"},"source":["# PVInspect integration with PyTorch\n","\n","PVInspect interfaces directly with PyTorch, since PVInspect `ImageSequence`s can be simply converted into PyTorch datasets. To demonstrate this, let's first set up everything:"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5808,"status":"ok","timestamp":1629900024449,"user":{"displayName":"Mathis Hoffmann","photoUrl":"","userId":"16668827174194590856"},"user_tz":-120},"id":"28ZA1FH9TrwN","outputId":"ac9d8f55-94fe-4f46-a6a3-c2e7b509ddd5"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/z003v8ys/dev/pvinspect/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import pvinspect as pv\n","from pvinspect import datasets\n","from pvinspect.data.image import DType\n","from pvinspect.integration.pytorch.dataset import ClassificationDataset\n","from torchvision.transforms import Compose, RandomHorizontalFlip, RandomVerticalFlip, ToTensor, ToPILImage"]},{"cell_type":"markdown","metadata":{"id":"RBiWM_HGUIAi"},"source":["Now, we load our sample dataset and only use the training data:"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6643,"status":"ok","timestamp":1629900031089,"user":{"displayName":"Mathis Hoffmann","photoUrl":"","userId":"16668827174194590856"},"user_tz":-120},"id":"gYIVTJ7OUDbq","outputId":"bc637da4-d04c-40e3-9e6e-d865922fef18"},"outputs":[{"data":{"text/plain":["2324"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["seq = datasets.elpv().pandas.query(\"testset == False\")\n","len(seq)"]},{"cell_type":"markdown","metadata":{"id":"C3fMmlb4UVsy"},"source":["Note that this has annotations available in form of boolean metra attributes:"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1629900031092,"user":{"displayName":"Mathis Hoffmann","photoUrl":"","userId":"16668827174194590856"},"user_tz":-120},"id":"iO59aa5hUPzq","outputId":"d77e8628-f97e-4f72-cf5f-7e57b6478cca"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>original_filename</th>\n","      <th>modality</th>\n","      <th>defect_probability</th>\n","      <th>wafer</th>\n","      <th>crack</th>\n","      <th>inactive</th>\n","      <th>blob</th>\n","      <th>finger</th>\n","      <th>testset</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>cell0382.png</td>\n","      <td>EL_IMAGE</td>\n","      <td>1.000000</td>\n","      <td>mono</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>cell2581.png</td>\n","      <td>EL_IMAGE</td>\n","      <td>0.333333</td>\n","      <td>poly</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>cell0396.png</td>\n","      <td>EL_IMAGE</td>\n","      <td>1.000000</td>\n","      <td>mono</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  original_filename  modality  defect_probability wafer  crack  inactive  \\\n","0      cell0382.png  EL_IMAGE            1.000000  mono   True     False   \n","1      cell2581.png  EL_IMAGE            0.333333  poly  False     False   \n","2      cell0396.png  EL_IMAGE            1.000000  mono   True      True   \n","\n","    blob  finger  testset  \n","0  False    True    False  \n","1  False   False    False  \n","2  False    True    False  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["seq[:3].meta"]},{"cell_type":"markdown","metadata":{"id":"OOZjw5hOUglk"},"source":["For training, we usually have some data augmentation pipeline. Let's set up a simple one:"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1629900031093,"user":{"displayName":"Mathis Hoffmann","photoUrl":"","userId":"16668827174194590856"},"user_tz":-120},"id":"MCyotbLXUaxZ"},"outputs":[],"source":["tfms = Compose([ToPILImage(), RandomHorizontalFlip(), RandomVerticalFlip(), ToTensor()])"]},{"cell_type":"markdown","metadata":{"id":"yzz_9yxLUosB"},"source":["Now, converting `seq` into a PyTorch classification dataset using the transforms is as simple as:"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1629900031095,"user":{"displayName":"Mathis Hoffmann","photoUrl":"","userId":"16668827174194590856"},"user_tz":-120},"id":"HoS72hEWUmCz","outputId":"b07c37a2-290b-4cce-8c5e-55c301708f74"},"outputs":[{"data":{"text/plain":["<pvinspect.integration.pytorch.dataset.ClassificationDataset at 0x2888a2830>"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["ds = ClassificationDataset(seq, meta_classes=[\"crack\", \"inactive\"], data_transform=tfms)\n","ds"]},{"cell_type":"markdown","metadata":{"id":"qtITkSTeU1hB"},"source":["With the `meta_classes` parameter, we specify the meta attributes that should be converted into one-hot class variables. We can now access individual elements from the dataset:"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":321,"status":"ok","timestamp":1629900031402,"user":{"displayName":"Mathis Hoffmann","photoUrl":"","userId":"16668827174194590856"},"user_tz":-120},"id":"QlqYB2AnUz0S","outputId":"f4d5da39-a0a2-405b-dd3a-b4e08a204916"},"outputs":[{"data":{"text/plain":["(tensor([[[0.1137, 0.1137, 0.1137,  ..., 0.0980, 0.1020, 0.1020],\n","          [0.1176, 0.1176, 0.1176,  ..., 0.1059, 0.1059, 0.1059],\n","          [0.1216, 0.1216, 0.1216,  ..., 0.1059, 0.1059, 0.1059],\n","          ...,\n","          [0.2157, 0.2157, 0.2157,  ..., 0.2235, 0.2196, 0.2157],\n","          [0.2118, 0.2157, 0.2157,  ..., 0.2235, 0.2196, 0.2157],\n","          [0.2118, 0.2157, 0.2157,  ..., 0.2196, 0.2157, 0.2157]]]),\n"," tensor([1., 0.]))"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["ds[0]"]},{"cell_type":"markdown","metadata":{"id":"TdBeJAF5VU-5"},"source":["Here, we see that\n","\n","\n","1.   Image data is corretcly converted into PyTorch tensors, which shows that the transforms are applied\n","2.   Specified classes are converted into a one-hot vector and returned as a second return value\n","\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOa5YRYP+dhAqpUBMaqkfdt","collapsed_sections":[],"name":"pyinspect_pytorch_integration.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
